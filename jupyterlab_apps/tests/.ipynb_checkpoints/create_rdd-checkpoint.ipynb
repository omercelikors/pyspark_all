{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/12 06:38:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session internally creates a sparkContext variable of SparkContext. \n",
    "# You can create multiple SparkSession objects but only one SparkContext per JVM. \n",
    "# In case if you want to create another new SparkContext you should stop existing \n",
    "# Sparkcontext (using stop()) before creating a new one.\n",
    "# Create SparkSession.\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"create_rdd\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "<class 'list'>\n",
      "[('Java', 20000), ('Python', 100000), ('Scala', 3000)]\n",
      "Java,20000\n",
      "Python,100000\n",
      "Scala,3000\n"
     ]
    }
   ],
   "source": [
    "# Create RDD from parallelize .   \n",
    "dataList = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\n",
    "rdd1=spark.sparkContext.parallelize(dataList)\n",
    "print(type(rdd1))\n",
    "# print(rdd) This does not work.\n",
    "# Usually, collect() is used to retrieve the action output \n",
    "# when you have a very small result set, and calling collect() \n",
    "# on an RDD with a bigger result set causes out of memory as it \n",
    "# returns the entire dataset (from all workers) to the driver \n",
    "# hence we should avoid calling collect() on a larger dataset.\n",
    "data_collected1=rdd1.collect()\n",
    "print(type(data_collected1))\n",
    "print(data_collected1)\n",
    "for row in data_collected1:\n",
    "    print(row[0] + \",\" +str(row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "<class 'list'>\n",
      "['Mehmet Kerimoğlu', 'Seven Kurtoğlu', 'Gizem Denli, Hakan Demsiz']\n",
      "M,e\n",
      "S,e\n",
      "G,i\n"
     ]
    }
   ],
   "source": [
    "# Create RDD from external data source.\n",
    "rdd2 = spark.sparkContext.textFile(\"/opt/data/tests/input_datas/football_players.txt\")\n",
    "print(type(rdd2))\n",
    "# print(rdd) This does not work.\n",
    "data_collected2=rdd2.collect()\n",
    "print(type(data_collected2))\n",
    "print(data_collected2)\n",
    "for row in data_collected2:\n",
    "    print(row[0] + \",\" +str(row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
