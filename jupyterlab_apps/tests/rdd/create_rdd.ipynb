{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/12 13:17:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session internally creates a sparkContext variable of SparkContext. \n",
    "# You can create multiple SparkSession objects but only one SparkContext per JVM. \n",
    "# In case if you want to create another new SparkContext you should stop existing \n",
    "# Sparkcontext (using stop()) before creating a new one.\n",
    "# Create SparkSession.\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"create_rdd\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()\n",
    "#spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[('Java', 20000), ('Python', 100000), ('Scala', 3000)]\n",
      "Java,20000\n",
      "Python,100000\n",
      "Scala,3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create RDD from parallelize .   \n",
    "dataList = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\n",
    "# sparkContext.parallelize([1,2,3,4,56,7,8,9,12,3], 10) with 10 partition\n",
    "rdd1=spark.sparkContext.parallelize(dataList)\n",
    "print(type(rdd1))\n",
    "# print(rdd) This does not work.\n",
    "# Usually, collect() is used to retrieve the action output \n",
    "# when you have a very small result set, and calling collect() \n",
    "# on an RDD with a bigger result set causes out of memory as it \n",
    "# returns the entire dataset (from all workers) to the driver \n",
    "# hence we should avoid calling collect() on a larger dataset.\n",
    "data_collected1=rdd1.collect()\n",
    "print(type(data_collected1))\n",
    "print(data_collected1)\n",
    "for row in data_collected1:\n",
    "    print(row[0] + \",\" +str(row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rdd2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create RDD from external data source.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m rdd1 \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mtextFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/data/tests/input_datas/football_players.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(\u001b[43mrdd2\u001b[49m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# print(rdd) This does not work.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m data_collected2\u001b[38;5;241m=\u001b[39mrdd2\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rdd2' is not defined"
     ]
    }
   ],
   "source": [
    "# Create RDD from external data source.\n",
    "rdd2 = spark.sparkContext.textFile(\"/opt/data/tests/input_datas/football_players.txt\")\n",
    "print(type(rdd2))\n",
    "# print(rdd) This does not work.\n",
    "data_collected2=rdd2.collect()\n",
    "print(type(data_collected2))\n",
    "print(data_collected2)\n",
    "for row in data_collected2:\n",
    "    print(row[0] + \",\" +str(row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads entire file into a RDD as single record.\n",
    "# wholeTextFiles() function returns a PairRDD with the key being the file path and value being file content.\n",
    "rdd3 = spark.sparkContext.wholeTextFiles(\"/opt/data/tests/input_datas/football_players.txt\")\n",
    "print(type(rdd3))\n",
    "# print(rdd) This does not work.\n",
    "data_collected3=rdd3.collect()\n",
    "print(type(data_collected3))\n",
    "print(data_collected3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates empty RDD with no partition.   \n",
    "rdd4 = spark.sparkContext.emptyRDD\n",
    "print(type(rdd4))\n",
    "# rddString = spark.sparkContext.emptyRDD[String]\n",
    "# print(type(rddString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty RDD with partition.\n",
    "rdd5 = spark.sparkContext.parallelize([],10) #This creates 10 partitions\n",
    "print(type(rdd5))\n",
    "# Get numbers of partition.\n",
    "print(\"Initial partition count:\"+str(rdd5.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition and Coalesce.\n",
    "# Note that repartition() method is a very expensive operation as it shuffles data from all nodes in a cluster. \n",
    "reparRdd = rdd5.repartition(4)\n",
    "print(\"re-partition count:\"+str(reparRdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
